import { ArticleLayout } from "@libs/blog/components/article-layout";

export const article = {
  author: "Ying Wang",
  date: "2017-10-18",
  title: "My Experience with Refactoring",
  description: "My Experience with Refactoring",
  categories: ["Software"],
};

export const metadata = {
  title: article.title,
  description: article.description,
};

export default (props) => (
  <ArticleLayout article={article} theme="solarized" {...props} />
);

You know that guy from ["That Happened"](/posts/2017/10/04/that_happened) and ["That Happened, Part 2"](/posts/2017/10/15/that_happened_part_2) and that `/qa-inactivity-timeout` route he made that does all that random stuff that should never see the light of day, let alone be an API endpoint? Apparently he increased the timeout from 1.1 seconds to 1000 minutes and merged that code into production without anybody competent knowing about it. So machine shops using our vendor portal who were logged out at the time that code hit our production servers are not allowed in anymore and instead are re-directed to our internal ERP login site.

[This is fine](/img/posts/2017/10/18/my_experience_with_refactoring_1.png)

---

While the company burns around me, I did a small celebration today because I "finished" refactoring the machine learning pipeline for our pricing algorithms. :tada:

I thought it would take me one week; instead, it took me about a month. It was my first major refactoring effort ever, and I think I'm fairly happy with how it turned out. It took this long because not only was there a decent amount of code to refactor, but all functionality must be preserved exactly as it was and new features added during this time must be accommodated. Also I had no idea how to data science or machine learn when I started out. :flushed:

Our machine learning pipeline is what some may call a "successful experiment". It works in pricing parts routed to it similarly to our old formulaic approach, but the code itself was not great. If we wanted to extend the functionality of our ML pipeline, we would need to restructure our code in order to support that.

---

The biggest worry I had coming out of the initial development of our pricing algorithms was that they would be given over to somebody else, somebody who doesn't care (:eyes:'), and that I would have to watch my baby slowly die. So I sought, and received, assurances from our CTO and data science lead that I would move into a research engineering position with data science and transition out of the armageddon that is web services. With that in place, I could refactor our code with a much longer perspective (?) than I could otherwise.

---

One thing immediately evident when I was starting off this refactoring effort was where to put all of our machine learning code. I was working in a repository separate from those of both our computational geometry and web services, because our pricing algorithms had become a hairball to maintain across both. However, geometry and web leads also worried about having too many top-level repositories in production. We don't have a `git submodule` or a `git subtree` trunking philosophy, so every repository has to be deployed individually (and as of a few weeks ago, management decided that developers should no longer have access to QA or production, so I am not in a position to fix that), and as our deployments are manual with no continuous integration, the complexity of our deployments scale with the number of repositories that need to be deployed. Additionally, the deep learning code must be easily accessible to other teams repositories for training and analysis.

I asked the data scientists creating the prototype to add the deep learning code to the `interfaces/` folder I made for interfaces to third-party packages. However, the amount of code present for our ML pipeline was such that placing it in `interfaces/` no longer made any sense. An interface is, by definition, thin. So what I ended up doing was creating a new `libraries/` folder and creating a `xometry_deep_learning` directory in there. The idea is that at any moment, that could be taken out and packaged into a pip wheel or a conda tarball as needed, with the repository trunked into the main pricing repository at the location specified. I do believe that moving from `interfaces/` to `libraries/` was an important distinction to make.

---

There were a number of tricky things I had to watch out for while refactoring this ML pipeline.

Machine learning is very finicky. It does not break when something goes wrong, it just returns bad results, and if you do not check those results, you will never know you made a mistake until somebody orders a complete rocket engine assembly for $5. Some of the changes I made (and eventually reverted) broke the regression without me knowing about it. For example, one tricky thing was that the order of inputs into the ML pipeline matters. We store this order as a ranked Python list. I decided to go ahead and chop up this list (which is at least a hundred distinct features) into bite-size components for better readability. I did not watch out for the order. Luckily for me, our data science lead caught my mistake while he was running a diff between the baseline and my changes, and notified me. After hunting for a while, I narrowed down the changes I made to this ranked list, and then I realized this thing to watch out for. So instead of breaking up this hundred-element-plus long list, I made it its own file and pasted it in. Then I made sure to run this regression and diff pretty much before every commit.

In addition, some changes cannot be made easily even if they do not technically break regression, because of output validation. For example, using `numpy.cbrt(some_number)` and using `some_number ** (1 / 3.0)` should return the same results. However, due to what I'm guessing is the implementation of the underlying floating point calculation beneath `numpy`, the values generated are slightly different (and by slightly I mean at the 7th decimal place), which means when you diff the values calculated by the ML pipeline between the baseline and the changes you made, you get a whole bunch of additions and deletions. That would be fine - until you realize you have more refactoring left to do, and if your diff is not perfect you have to stop refactoring right there for fear that you would make a change that your diff could not catch anymore. I left that one as-is.

The final tricky thing I had to watch out for is performance. I did not want any slowdown in pricing associated with this refactoring effort whatsoever. The problem is that the times associated with ML computation varies by some amount. Our initialization process takes about ~60ms, which is nothing in production as that is amortized over many different calls, but difficult to keep track of locally when running a new regression every time. This initialization time takes a different amount every time and varies based on things like CPU usage by other applications. I haven't found a good way around this. I still have to look at a generated chart to ensure that the times are within reason. This saved me on at least one occasion, when I made a class initialization around a pickled object that would have made pricing times 6x slower. Instead, I kept it as a top-level method and stored the restored pickle as a singleton.

---

You can never finish refactoring. Much of the code right now that I consider to be "finished" is not actually finished. I cut and pasted the code in functional blocks and added TODO statements at the top that would get picked up by my text editor or somebody else's IDE. I can't touch it without changing functionality because there were and are no unit tests that cover every conditional.

Speaking of unit tests, that's the next phase: adding back enough unit tests to the more convoluted classes present in our ML pipeline so that I can break them up. I separated out my coverage reports by library, which as it stands now is the core and the ML pipeline, and discovered that I have only about 20% coverage in some critical files. I will need to boost those up to a useful value before being able to refactor.

---

Was it worth it? After our latest ML pipeline extension, I'd say that's an absolute yes. Data science was able to follow the existing structure of the code and build much cleaner code with minimal assistance. With this precedent, I'd say the returns on this effort will keep coming for the foreseeable future.

Thanks to Dwipam Katariya, Yuan Chen, Mark Wicks, Will Sankey, and Eric Sinzinger for providing the support I needed to make this happen.
