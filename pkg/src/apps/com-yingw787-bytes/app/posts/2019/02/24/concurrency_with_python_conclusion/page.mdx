import { ArticleLayout } from "@libs/blog/components/article-layout";

export const article = {
  author: "Ying Wang",
  date: "2019-02-24",
  title: "Concurrency with Python: Conclusion",
  description: "Concurrency with Python: Conclusion",
  categories: ["Python", "Concurrency"],
};

export const metadata = {
  title: article.title,
  description: article.description,
};

export default (props) => (
  <ArticleLayout article={article} theme="default" {...props} />
);

The Concurrency with Python Series:

- [Concurrency with Python:
  Why?](/posts/2019/01/11/concurrency_with_python_why/)
- [Concurrency with Python: Threads and
  Locks](/posts/2019/01/12/concurrency_with_python_threads_and_locks/)
- [Concurrency with Python: Functional
  Programming](/posts/2019/01/19/concurrency_with_python_functional_programming/)
- [Concurrency with Python: Separating Identity From
  State](/posts/2019/01/26/concurrency_with_python_identity_and_state/)
- [Concurrency with Python: Actor
  Models](/posts/2019/02/02/concurrency_with_python_actor_models/)
- [Concurrency with Python: CSP and
  Coroutines](/posts/2019/02/09/concurrency_with_python_csp_and_coroutines/)
- [Concurrency with Python: Hardware-Based
  Parallelism](/posts/2019/02/16/concurrency_with_python_hardware_based_parallelism/)
- [Concurrency with Python: Data-Intensive
  Architectures](/posts/2019/02/23/concurrency_with_python_data_intensive_architectures/)
- [**Concurrency with Python:
  Conclusion**](/posts/2019/02/24/concurrency_with_python_conclusion/)

---

**(Update 07/07/2019)**: This series was intended to be a starting off point for
a larger discussion around Python concepts. However, it turns out writing
in-depth technical overviews of concurrency models in Python is considerably
more difficult than I anticipated :flushed:

Original estimates averaged about half a week from research to publication per
blog post. In actuality it's taken about two to three full weeks per blog post,
and 2-4 hours a day at that. I've learned a lot, but I'm also ready to try
something else to give back to the software community and improve my own
professional career. Feel free to [email me](mailto:me@yingw787.com) if you have
any advice/tips.

---

## Recap

This series began in part as an attempt to answer questions about how
[concurrency models behave when combined
together](/posts/2019/01/11/concurrency_with_python_why). In doing so, it
covered:

- [Threads and
  locks](/posts/2019/01/12/concurrency_with_python_threads_and_locks/), where
  developers interface more or less directly with the hardware.

- [Functional
  programming](/posts/2019/01/19/concurrency_with_python_functional_programming/),
  where source code is written in an idempotent and commutative way to allow
  either the developer or the language to schedule tasks concurrently.

- [Separation of identity and
  state](/posts/2019/01/26/concurrency_with_python_identity_and_state/), where
  the language itself supports structs where atomic snapshots can be taken
  safely.

- [Actor models](/posts/2019/02/02/concurrency_with_python_actor_models/),
  where developers constrained their code to fit the framework of sending and
  receiving messages.

- [Communicating sequential
  processes](/posts/2019/02/09/concurrency_with_python_csp_and_coroutines/),
  where channels share information between state machines concurrently
  executing tasks.

- [Hardware-based
  parallelism](/posts/2019/02/16/concurrency_with_python_hardware_based_parallelism/),
  where specialized hardware enabling parallelism provided a software
  framework to move data from one place to another.

- [Data-intensive
  architectures](/posts/2019/02/23/concurrency_with_python_data_intensive_architectures/),
  where commodity hardware leveraged software frameworks and other concurrency
  models to execute operations on large quantities of data.

## Lessons

I've found as I've worked through this series that I gained more questions than
answers, but that there are some heuristics you can always apply when designing
systems at scale:

- **Focus on the properties of the language**: Fundamental programming
  language aspects like the type system (e.g. encodings and byte definitions
  of types, type system richness and extensibility), attribute defaults (e.g.
  immutability and identity of data structures), and control flow primitives
  will strongly affect what kinds of concurrency paradigms are appropriate.
  This is essentially describing the field of [programming language
  theory](https://en.wikipedia.org/wiki/Programming_language_theory), and this
  series is effectively detailing why it's important.

  Generally, decisions that can be formally proved are better than those that
  cannot. Mark Semann published [a great blog post about this reason
  propelling his move from object-oriented to functional
  languages](https://blog.ploeh.dk/2017/10/04/from-design-patterns-to-category-theory/),
  since functional languages relate to [category
  theory](https://en.wikipedia.org/wiki/Category_theory) while object-oriented
  languages relate to design patterns, which is more of a heuristic. Along
  that axis of exploration, learning about how languages implement [process
  calculuses](https://en.wikipedia.org/wiki/Process_calculus) may shine light
  into how tightly defined a language implements a concurrency model.

- **The simplest abstractions (to the hardware) are the ones that thrive**:
  Originally, I thought the simplest abstractions to the developer would be
  the ones that thrive. If you took [a monkey, a typewriter, and the ability
  to write code](https://en.wikipedia.org/wiki/Infinite_monkey_theorem), you
  could produce pretty much any program in that language, and imagining it as
  such makes it easy to see how most software is just bugs with a few happy
  paths sprinkled in. Hence, simple abstractions that are easy to grok should
  provide the most reliable guarantees of correctness, the smallest
  organizational overhead, and the happiest developers. Right?

  Not necessarily. While strongly opinionated languages like Erlang or Clojure
  may [attract the best developers](http://www.paulgraham.com/pypar.html), the
  languages with broad corporate support are the ones that give developers
  lots of access and power. While threading/locking code is difficult to
  write, using threads and locks instead of actors might eke out a tiny
  performance gain, which may result in millions of dollars of savings at
  scale, which results in continued corporate investment into the language
  that made it possible. Hence, the positive spiral of success.

- **Concurrency models, as with all software models, come with different
  tradeoffs**: Many of these models do not play nice with each other, and
  require a service layer or other intermediary in order to communicate with
  each other. In addition, many newcomers to a language come from different
  engineering backgrounds, and since learning a language to become
  production-ready is difficult enough, tying a particular model to a language
  increases complexity in learning past the threshold of feasibility.

  This is likely why most general-purpose languages and toolchains don't
  recommend a single concurrency model, but instead offer a wide suite of
  options and make it work. With a "way out", the model loses much of its
  strong guarantees, but developers gain a lot of flexibility in shipping
  results.

<br />
